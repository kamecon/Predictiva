---
title: "Resumen laboratorios de Regresión Lineal"
author: "Kamal Romero"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  rmdformats::readthedown:
    highlight: kate
  html_document: 
    toc: true
    number_sections: true
    theme: "yeti"
  md_document:
    variant: markdown_github
  pdf_document: 
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE)
```

# Laboratorio 1

Lo primero que debemos hacer **siempre** es cargar o instalar las librerías que necesitemos para nuestro trabajo.

En este curso hemos usado la librería `pacman`, la cual no solo carga la librería sino que la instala en caso que el usuario no la tenga instalada. Esto nos ahorra no solo los pasos de instalar y cargar, sino además instala la librería.

Verificamos si esta instalado la libreria `pacman`, en caso que no lo este, se instala

```{r lib01}
if (! ('pacman' %in% installed.packages())) install.packages('pacman')
```

Una vez instalada la librería `pacman`, cargamos (o instalamos) las librerías necesarias para este laboratorio

```{r lib02}
pacman::p_load("haven", "AER")
```

## Primer ejemplo del libro de Stock y Watson

### Dos formas de cargar los datos

La primera es accediendo a la web del libro y descargar los datos. Acceder a esta [dirección](https://wps.pearsoned.com/aw_stock_ie_3/178/45691/11696965.cw/index.html) y pinchar en *"Replication Files"* en el menu de la barra izquierda, luego pinchar en *"Datasets for Replicating Empirical Results (Original Edition)"*. Descargar los datos *"California Test Score Data Used in Chapters 4-9"* en formato STATA.

Vamos a practicar con el formato de STATA por dos motivos: debemos acostumbrarnos a trabajar con distintos formatos de datos mas alla del csv y xlsx, y porque STATA (de momento) sigue siendo el software estadístico predominante en organimos multilaterales y gubernamentales economicos, asi como en gran parte de la investigacion academica en economia

Una vez descargados los datos, lo leemos con la libreria haven 

```{r sw_1_01}
caschool <- read_dta("Data/caschool.dta")
```

Vamos a replicar la regresion lineal del capitulo 4 trabajada en clase, la cual explicaba la calificación de la prueba de habilidad numerica y verbal en funcion del numero de alumnos por profesor

En R la funcion `lm()` lleva a cabo la regresion lineal, veamos la ayuda

```{r sw_1_02, eval=FALSE}
?lm
```

De momento nos concentramos en los primeros dos argumentos de la funcion, formula y data


```{r sw_1_03}
lm(data = caschool, formula = testscr ~ str)
```

Vemos que con el argumento data de la función `lm`, indicamos de que fuente de datos se deben obtener las variables, otra forma es no indicar dentro de la funcion la fuente de los datos, y usar la funcion `attach` previo a `lm`

```{r sw_1_04}
attach(caschool)
lm(testscr ~ str)
```


Asimismo, es deseable referenciar (asignar un nombre) el objeto que surge de la función `lm`

```{r sw_1_05}
estimacion_01 <- lm(testscr ~ str)
estimacion_01
```

Más adelante aprenderemos más acerca de la funcion `lm`


Para reproducir el gráfico del libro, usamos las funciones `plot` y `abline`

```{r sw_1_06}
plot(caschool$str, caschool$testscr)
abline(estimacion_01)

```

La función `abline` añade una línea recta a un gráfico, en este caso añade la línea que corresponde a los parámetros estimados por la función `lm` que hemos referenciado en `estimacion_01`

Mas parecido al gráfico del libro

```{r sw_1_07}
plot(caschool$str, caschool$testscr,
     ylim = c(600,720),   #limites del eje y
     xlim = c(10,30),     #limites del eje x
     pch = 20,            #forma del simbolo (circulo)
     cex = 1.5,           #tamaño de la forma
     ylab = "Test score", #titulo del eje y
     xlab = "Student-teacher ratio", #titulo del eje x
     axes = FALSE)        #Sin ejes
axis(1) #eje x
axis(2) #eje y
abline(estimacion_01, col = "deepskyblue", lwd =3)

```

Si leen el documento de características de los datos, podran ver que las dos variables que hemos usado no estaban en los datos originales, sino que se han construido

Esto es algo que deben hacer continuamente en cualquier trabajo empirico, por lo que deberian practicarlo

Para esto, vamos a acceder los mismos datos de otra fuente, la libreraia AER (Applied Econometrics with R), accedamos al Vignette

```{r sw_1_08}
vignette("AER")
```

Veamos las bases de datos del libro de Stock y Watson (el resultado aparece en la pestaña de help)


```{r sw_1_09, warning=FALSE}
help("StockWatson2007", package = "AER")
```

Cargamos los datos 

```{r sw_1_10}
data(CASchools)
```

Construimos las variables y estimamos el modelo

```{r sw_1_11}
CASchools$STR <- CASchools$students/CASchools$teachers

CASchools$Testscr <- (CASchools$math+CASchools$read)/2

lm(formula = Testscr ~ STR, data = CASchools)

```

## Regresion Lineal: Población vs muestra

El objetivo de la siguiente sección es replicar lo que ocurre si estimamos varias veces el mismo proceso, pero con distintas realizaciones del ruido. Aunque la analogía no es exacta, esto es algo similar a lo que ocurre si tomamos muestras repetidas de una población determinada, volveremos a esto en un próximo laboratorio. Ahora, lo que veremos, es que aún teniendo el modelo verdadero, el hecho de que exista un ruido en dicho modelo, hace que nuestras estimaciones sean solo una de muchas posibles

Vamos a replicar la figura 3.3 del libro [An Introduction to Statistical Learning](https://www.statlearning.com/)

Creamos la función del proceso generador de datos $Y=2+3x$

```{r sw_1_11_2}
x <- seq(-2, 2, length = 100)
y <- 2 + 3*x
```


A partir de 100 realizaciones de una normal con media cero y desviación típica (`dt`) 1, creamos el modelo $Y=f(x)+u$, donde $f(x)=2+3x$. Almacenamos los valores de $x$, $y$ y el nuevo $y$ estimado en un data frame `df` 

```{r sw_1_12}
set.seed(1234)
dt <- 1
y_02 <- y + rnorm(n = 100, mean = 0, sd = dt)
df <- data.frame(x,y,y_02)
```

Estimamos el modelo a partir de los datos con ruido

```{r sw_1_13}
fit1 <- lm(formula = y_02 ~ x, data = df)
fit1
```


Repetimos el procedimiento anterior con 100 realizaciones distintas del error y estimamos los parámetros

```{r sw_1_14}
y_03 <- y + rnorm(n = 100, mean = 0, sd = dt)
df$y_03 <- y_03
fit2 <- lm(formula = y_03 ~ x, data = df)
fit2
```


Lo hacemos de nuevo

```{r sw_1_15}
y_04 <- y + rnorm(n = 100, mean = 0, sd = dt)
df$y_04 <- y_04
fit3 <- lm(formula = y_04 ~ x, data = df)
fit3
```


Observamos que las estimaciones de la ordenada en el origen y la pendiente cambian con distintas realizaciones del ruido

Graficamos el proceso generador de datos "real" con las estimaciones de la regresión lineal

```{r sw_1_16}
plot(x,y_02,xlim=c(-2,2), ylim=c(-10,10), ylab = "Y", xlab = "X", yaxs="i", xaxs="i", main = "Figura 3.3 Libro ISLR")
lines(x,y, col="red", lwd=4)
lines(x, fitted(fit1), col=colors()[10*1], lwd=2)
```


Vamos a generar 10 nuevos modelos repitiendo lo anterior

Creamos un data frame vacio de 100 filas (x's) y 10 columnas (modelos), para rellenarlo luego con las distintas versiones del modelo
```{r sw_1_17}
df2 <- data.frame(matrix(, nrow=100, ncol=10))
```

Añadimos una columna con la variable X

```{r sw_1_18}
df2$x <- x
```

Generamos los 10 distintos conjuntos de datos a partir del proceso generador de datos y el ruido

Primero la columna Y
```{r sw_1_19}
for (i in 1:10) {
  df2[,i] <- y + rnorm(100, 0, dt)
}
```


Luego estimamos los 10 modelos y lo representamos graficamente
```{r sw_1_20}
plot(NULL, xlim=c(-2,2), ylim=c(-10,10), ylab = "Y", xlab = "X", yaxs="i", xaxs="i", main = "Figura 3.3 Libro ISLR")
for (i in 1:10) {
  fitt <- lm(data = df2, formula = df2[,i]~x)
  lines(x, fitted(fitt), col=colors()[12*i])
}
```


# Laboratorio 2

```{r sw_2_01}
pacman::p_load("haven", "wooldridge")
```

## Practica regresion lineal 

**Nota:** parte de esta sección se desarrolla con más detalle en el html *Regresion Lineal Simple en R*

Volvemos al ejemplo del libro de Stock y Watson trabajado en la sección anterior

```{r sw_2_02}
caschool <- read_dta("Data/caschool.dta")

estimacion_01 <- lm(formula = testscr ~str, data = caschool)

summary(estimacion_01)

```


Accedemos a los coeficientes estimado del modelo

```{r sw_2_03}
estimacion_01$coefficients
estimacion_01$coefficients[1]
estimacion_01$coefficients[2]

coef(estimacion_01)

```


**Adicional: Regresión robusta** Esto se desarrolla con más detalle en el html *Regresión lineal simple con errores robustos a heterocedasticidad*

```{r sw_2_04}
pacman::p_load("lmtest", "sandwich")

coeftest(estimacion_01, vcov = vcovHC(estimacion_01, type = "HC1"))

```


Accedemos a mas informacion ($R^2$, ESR), para esto accedemos a los atributos de la función `summary`

```{r sw_2_05}
attributes(summary(estimacion_01))

summary(estimacion_01)$r.squared
summary(estimacion_01)$sigma

```


Construimos los residuos

```{r sw_2_06}
bhat <- estimacion_01$coefficients
yhat <- estimacion_01$coefficients[1] + estimacion_01$coefficients[2]*caschool$str
residuo <- caschool$testscr - yhat

```

Lo representamos gráficamente para verificar que nuestros residuos estimados coinciden con los estimados por la función `lm`

```{r sw_2_07}
plot(residuo, type = "l")
lines(estimacion_01$residuals, col="red")

```

Obtenemos las predicciones del modelo y los residuos, los organizamos en una tabla

```{r sw_2_08}
Str <-  caschool$str
Test_scr <-  caschool$testscr
testscr_hat <- estimacion_01$fitted.values
u_hat <- estimacion_01$residuals

```

Donde `Test_scr` es el valor observado de las calificaciones de la prueba de aptitud, y `testscr_hat` es la estimación de las calificaciones del modelo. El residuo `u_hat` es la diferencia entre ambas.

Usamos el comando `cbind()` para unir las columnas (vectores) de datos que hemos creado en el paso anterior

```{r sw_2_09}
tabla01 <- cbind(Str, Test_scr, testscr_hat, u_hat)

head(tabla01)

```

A continuación, replicamos algunos de los ejemplos del libro de Wooldridge

### Ejemplo 2.3 libro Wooldridge

```{r sw_2_10}
data(ceosal1)

estimacion_wool01 <- lm(formula = salary ~roe, data = ceosal1)
estimacion_wool01

plot(ceosal1$roe, ceosal1$salary, xlab = "ROE", ylab = "Salario de los CEO's")
abline(estimacion_wool01)

```

Los siguientes dos ejemplos deben completarlos siguiendo el paso anterior

### Ejemplo 2.4 libro Wooldridge: Relacion entre salario (wage) y educacion (educ) (Tarea: inspeccionar los datos)


```{r sw_2_11, eval=FALSE}
data("wage1")

estimacion_wool02 <- lm()
estimacion_wool02

plot(, xlab = , ylab = )
abline(estimacion_wool02)

```


### Ejemplo 2.5 libro Wooldridge: Relacion entre resultado de las elecciones (voteA) y el gasto en la campaña electoral (shareA)


```{r sw_2_12, eval=FALSE}
data("vote1")

?vote1

```


## Distribucion muestral de los betas 

Al igual que en su curso pasado de estadística inferencial, en el cual se estimaba un parámetro poblacional desconocido a partir de una muestra usando un estadístico, en este caso hacemos lo mismo cuando estimamos los parámetros de un modelo lineal.

Del mismo modo que podíamos construir una distribución de estadísticos muestrales, partiendo del supuesto de que tomabamos muestras repetidas de una población dada, podemos crear una distribución de los betas estimados en un modelo lineal.

A continuación vamos a tomar sub-muestras de las base de datos de caschool, y estimamos el modelo lineal $Testscr=\beta_0+\beta_1STR+u$ en cada una de ellas

Comenzamos creando un vector vacio de betas, cuya dimesión es igual al número de sub-muestras (y veces que estimamos el modelo) que tomamos

```{r sw_2_13}
beta_1 <- vector(length = 1000)
```

Para tomar una muestra (en este caso de tamaño 100) de un data frame, hacemos lo siguiente

```{r sw_2_14}
sample(nrow(caschool),100)
caschool[sample(nrow(caschool),100), ]

```


Rellenamos el vector con los betas estimados tomando solo una muestra de los datos originales

```{r sw_2_15}
for (i in seq_along(beta_1)) {
  estima <- lm(data =  caschool[sample(nrow(caschool),100),], formula = testscr ~str)
  beta_1[i] <- estima$coefficients[2]
}

mean(beta_1)
hist(beta_1)

```


**NOTA DE R**: En lugar de la función `seq_along`se puede usar una notación más convencional en otros lenguajes como `for (i in 1..1000)` o `for (i in 1:1000)` o `for i in range(1000)`, que significa "repite esta operación en índices que van del 1 al 1000 en saltos de 1" que es una manera sofisticada de decir "repite esta operación 1000 veces" 

**NOTA DE R**: En lugar de usar un bucle se puede usar la función `replicate`, una de las funciones de la familia `apply`, las cuales aplican una función a un vector de valores, que es lo mismo que hace un bucle cuando _aplica_ de manera repetida la misma operación a un conjunto de valores representado por un vector. En este caso estamos haciendo algo más sencillo, repetir un número de veces una instrucción encapsulada en una función, en este caso que nos extraiga de manera repetida una muestra de una población y calcule una regresion. Así que `replicate` en nuestro ejemplo repite 1000 veces la operación descrita arriba.

Este tipo de operaciones se realiza muchas veces en estadística, ya que de esa manera hacemos _simulaciones_, repetir muchas veces realizaciones de una distribución de probabilidad o un fenómeno cuyo resultado sea incierto (lanzar un dado 50000 veces)

```{r sw_2_16}
betas_1_2 <- replicate(expr = lm(data =  caschool[sample(nrow(caschool),100),], formula = testscr ~str)$coefficients[2],
                          n = 1000)

mean(betas_1_2)
hist(betas_1_2)

```

# Laboratorio 3

```{r sw_3_01}
pacman::p_load("haven", "wooldridge")
```

## Regresion lineal: parámetros

**Nota:** parte de esta sección se desarrolla con más detalle en el html *Regresion Lineal Simple en R*

Volvemos al ejemplo del libro de Stock y Watson trabajado en las secciones anteriores

```{r sw_3_02}
caschool <- read_dta("Data/caschool.dta")

estimacion_01 <- lm(formula = testscr ~str, data = caschool)

summary(estimacion_01)

```

Ya sabemos como acceder a los betas, el ESR y el $R^2$. Ahora vamos a acceder a los errores estándar de los parámetros, el t-valor y el p-valor

Volvemos a los atributos de la función `summary`

```{r sw_3_03}
attributes(summary(estimacion_01))
```

Inspeccionamos el atributo coefficients

```{r sw_3_04}
summary(estimacion_01)$coefficients
```


Podemos ver que es una matriz de dimension 2,4

```{r sw_3_05}
class(summary(estimacion_01)$coefficients)
dim(summary(estimacion_01)$coefficients)

```

Sabiendo esto, podemos extraer los elementos deseados

Error estándar de $\beta_0$

```{r sw_3_06}
summary(estimacion_01)$coefficients[1,2]
```

Error estándar de $\beta_1$

```{r sw_3_07}
summary(estimacion_01)$coefficients[2,2]
```

Estadístico t para contrastar que $\beta_1$ es 0

```{r sw_3_08}
summary(estimacion_01)$coefficients[2,3]
```

p-valor del contraste de $\beta_1$

```{r sw_3_09}
summary(estimacion_01)$coefficients[2,4]
```

**NOTA**: Si desea comparar el estadístico t con el valor critico, lo puede hacer con la funcion `qt()`

Al 99% de confianza (0.05 de probabilidad en cada cola en el caso del contraste bilateral), el estadistico de contraste seria


```{r sw_3_10}
qt(p = 0.995, df = 418)
```

Una manera de comapararlo

```{r sw_3_11}
t_valor <- summary(estimacion_01)$coefficients[2,3]
contraste <- qt(p = 0.995, df = 418)
ifelse(abs(t_valor)>abs(contraste),"Rechaza","Acepta")

```


Intervalos de confianza para $\beta_1$


```{r sw_3_12}
confint(estimacion_01)
```

Tambien se podria calcular "manualmente" lo anterior. Para eso, necesitariamos el numero de observaciones


```{r sw_3_13}
n <- nobs(estimacion_01)
n

```


# Laboratorio 4

```{r sw_4_01}
pacman::p_load("haven", "wooldridge")
```


## Regresion lineal multiple: motivación

Volvemos al ejemplo del libro de Stock y Watson trabajado en las secciones anteriores, y añadimos una variable adicional, el nivel de ingreso promedio del distrito escolar

```{r sw_4_02}
caschool <- read_dta("Data/caschool.dta")

modelo_cas01 <- lm(data = caschool, testscr ~ str)
summary(modelo_cas01)

modelo_cas02 <- lm(data = caschool, testscr ~ str + avginc)
summary(modelo_cas02)

```

Vemos como al añadir un nuevo regresor cambia el valor del parámetro $\beta_1$ que denota el efecto de *STR* sobre la calificación del examen

## Ejemplo 3.2 libro Wooldridge pag. 76 (salario real por hora)

526 observaciones sobre trabajadores en la base de datos WAGE1, las variables educ (años de educación), exper (años de experiencia en el mercado laboral) y tenure (años de antigüedad en el empleo actual) se incluyen en una ecuación para explicar log(wage)

```{r sw_4_03}
data("wage1")

estimacion_wage <- lm(data = wage1, log(wage) ~ educ + exper + tenure)
summary(estimacion_wage)

```


Estadistico de contraste

```{r sw_4_04}
qt(p = 0.975, df = 522)
```

p-valor

```{r sw_4_05}
pt(2.391, df = 522, lower.tail = FALSE)*2
```

## Regresión lineal multiple en notacion matricial

Ejemplo sacado de este [libro](http://www.urfie.net/downloads03.html) (sería interesante revisar este libro para replicar en R los ejemplos del libro de Wooldridge)

```{r sw_4_06}
data(gpa1)
```

Tamaño de la muestra y numero de regresores

```{r sw_4_07}
n <- nrow(gpa1)
k <- 2

```

Extraemos la variable independiente $y$

```{r sw_4_08}
y <- gpa1$colGPA
```

Construimos la matriz de regresores $X$ (en este caso 2) y añadimos una columna de unos (ver diapositiva 9)

```{r sw_4_09}
X <- cbind(1, gpa1$hsGPA, gpa1$ACT)
```

Inspeccionamos las primeras filas de  $X$:

```{r sw_4_10}
head(X)
```

Estimamos los betas usando la expresión de la diapositiva 18:

```{r sw_4_11}
beta_hat <- solve( t(X)%*%X ) %*% t(X)%*%y 
beta_hat

```

Calculamos los residuos

```{r sw_4_12}
uhat <- y - X %*% beta_hat
```

Calculamos la varianza estimada de $u$ y el ESR:

```{r sw_4_13}
sigsqhat <- as.numeric( t(uhat) %*% uhat / (n-k-1) )
ESR <- sqrt(sigsqhat) 
sigsqhat
ESR

```


Varianza estimada de los betas y su error estándar:

```{r sw_4_14}
Vbetahat <- sigsqhat * solve( t(X)%*%X )
Vbetahat
seBeta_hat <- sqrt( diag(Vbetahat))
seBeta_hat

```


Verificamos los valores con los resultados de la estimación usando `lm`

```{r sw_4_15}
modelo_gpa01 <- lm(data = gpa1, formula = colGPA ~ hsGPA + ACT )
summary(modelo_gpa01)

```

# Laboratorio 5

El objetivo de este laboratorio es aprender el uso de la función `summ`de la librería `jtools`, que aporta un resumen de la estimación del modelo visualmente más agradable y parecida a los resumenes que se suelen encontrar en publicaciones académicas e informes corporativos.

Cargamos las librerías 

```{r sw_5_01}
pacman::p_load(haven, jtools, vtable, huxtable)
```

Usamos el ejemplo del libro de Stock y Watson con el cual buscan motivar la regresión lineal multiple. Cargamos los datos y vemos como con la función `vtable` podemos ver un resumen de las variables del data frame

```{r sw_5_02}
caschool <- read_dta("Data/caschool.dta")

vtable(caschool)

```

Estimamos el modelo

```{r sw_5_03}
modelo_cas01 <- lm(data = caschool, testscr ~ str)
summary(modelo_cas01)

```

En lugar de usa summary, vamos a usar la funcion `summ` de la libreria `jtools`

```{r sw_5_04}
summ(modelo_cas01)
```

Si deseamos estimaciones robustas de los errores estándar, lo indicamos con el argumento `robust`

```{r sw_5_05}
summ(modelo_cas01, robust = "HC1")
```

Siguiendo el ejemplo del libro, vamos a introducir una nueva variable, el porcentaje de estudiantes que son estudiantes de inglés (es decir, estudiantes para quienes el inglés es un segundo idioma)

```{r sw_5_06}
modelo_cas03 <- lm(data = caschool, testscr ~ str + el_pct)
summ(modelo_cas03)

```

Vamos a usar errores robustos a heterocedasticidad, para que los resultados sean comparables a los del libro

```{r sw_5_07}
summ(modelo_cas03, robust = "HC1")
```

Incluimos el intervalo de confianza en la salida de la estimacion

```{r sw_5_08}
summ(modelo_cas03, robust = "HC1", confint = TRUE)
```

Si se desea incluir mas decimales lo hacemos con el argumento `digits`

```{r sw_5_09}
summ(modelo_cas03, robust = "HC1", confint = TRUE, digits = 3)

```

Otra manera de presentar el resumen de la estimación con la función `export_summs`

```{r sw_5_10, warning=FALSE}
export_summs(modelo_cas03)
```

Con errores robustos

```{r sw_5_11}
export_summs(modelo_cas03, robust="HC1")
```

Este formato es util si deseamos comparar modelos

```{r sw_5_12}
export_summs(modelo_cas01, modelo_cas03, robust=TRUE)
```

También podemos asignar nombres a las variables, asi se entiende mejor la estimaciòn al presentarla a un tercero

```{r sw_5_13}
export_summs(modelo_cas01, modelo_cas03,
             robust="HC1",
             coefs = c("Intercepto" = "(Intercept)",
                       "Alumnos por profesor" = "str",
                       "% de estudiantes de inglés" = "el_pct")
             )

```


Y cambiar el nombre de los modelos

```{r sw_5_14}
export_summs(modelo_cas01, modelo_cas03,
             robust="HC1",
             coefs = c("Intercepto" = "(Intercept)",
                       "Alumnos por profesor" = "str",
                       "% de estudiantes de inglés" = "el_pct"),
             model.names = c("Modelo 1", "Modelo 2")
            )

```

# Laboratorio 6

## Multicolinealidad

Vamos a replicar el ejemplo de la sección de multicolinealidad del libro [An Introduction to Statistical Learning](https://www.statlearning.com/) (páginas 100-102)

Cargamos las librerías
```{r sw_6_01}
pacman::p_load(ISLR, ISLR2, wooldridge, jtools, patchwork)
```

Vamos a trabajar con el conjunto de datos `Credit` de la librería `ISLR2`. Veamos la ayuda e inspeccionemos los datos

```{r sw_6_02}
?Credit
```


```{r sw_6_03}
head(ISLR2::Credit)
```

Replicamos el gráfico 3.14 del libro


```{r sw_6_04}

plot(Credit$Limit, Credit$Age, xlab = "Limit", ylab = "Age", col="red") + plot(Credit$Limit, Credit$Rating, xlab = "Limit", ylab = "Rating", col="red")

```

Se observa que Limit y Rating están altamente correlacionadas mientras que Limit y Age no lo están

Con la función `plot` usando como argumento el dataframe, se pueden ver todas las correlaciones a pares

```{r sw_6_05}
plot(Credit)
```

Reporducimos la tabla 3.11

```{r sw_6_06}
model_1 <- lm(data = Credit, formula = Balance ~ Age + Limit)
model_2 <- lm(data = Credit, formula = Balance ~ Rating + Limit)
export_summs(model_1,model_2, digits = 3)

```

Se observa como en la segunda regresión en la que se incluyen las variables altamente correladas, el error estándar del coeficiente de Limit aumenta considerablemente en relación a la segunda regresión, hasta el punto que deja de ser estadísticamente significativa.

## Variables cualitativas

Vamos a introducir una variable cualitativa en el modelo, copiamos el data frame de Credit en uno nuevo, para poder realizar modificaciones


```{r sw_6_07}
credito <- ISLR2::Credit
```

Creamos una variable binaria para indicar si la persona es propietaria de su vivienda o no, asignando un 1 en el caso que sea propietario y cero en caso contrario. Lo hacemos usando la función `ifelse`

```{r sw_6_08}
credito$propietario <- ifelse(credito$Own == "Yes", 1, 0)
```

Ahora estimamos el modelo con la variable cualitativa

```{r sw_6_09}
model_3 <- lm(data = credito, formula = Balance ~ propietario)
summ(model_3)
```

```{r sw_6_10}

```





wooldridge::wage1

head(wage1)

model_04 <- lm(data = wage1, formula = wage ~ female + educ + exper + tenure)
summ(model_04)

## Ejemplo pag. 97 ISLR

```{r sw_6_}

```

head(Credit)

```{r sw_6_}

```

model_b <- lm(data = Credit, formula = Balance ~ Income + Student)
summ(model_b, digits = 4)

```{r sw_6_}

```

credito <- Credit

credito$estudiante <- ifelse(credito$Student == "Yes", 1, 0)

head(credito)

```{r sw_6_}

```

model_b2 <- lm(data = credito, formula = Balance ~ Income + estudiante)
summ(model_b2, digits = 4)

## Interacciones

```{r sw_6_}

```

model_b3 <- lm(data = credito, formula = Balance ~ Income + estudiante + Income*estudiante)
summ(model_b3, digits = 4)

```{r sw_6_}

```

Advertising <- read.csv("Intro/Data/Advertising.csv")

```{r sw_6_}

```

modelo_publicidad <- lm(data = Advertising, formula = sales ~ TV + radio + newspaper)
summ(modelo_publicidad, digits = 4)

```{r sw_6_}

```

modelo_publicidad2 <- lm(data = Advertising, formula = sales ~ TV + radio)
summ(modelo_publicidad2, digits = 4)

```{r sw_6_}

```

modelo_publicidad3 <- lm(data = Advertising, formula = sales ~ TV + radio + TV*radio)
summ(modelo_publicidad3, digits = 4)

```{r sw_6_}

```



```{r sw_6_}

```

ggPredict(modelo_publicidad3, se = FALSE, interactive = TRUE)

ggPredict(model_3, se = FALSE, interactive = TRUE)

ggPredict(model_b, se = FALSE, interactive = TRUE)

```{r sw_6_}

```

model_041 <- lm(data = wage1, formula = wage ~  educ + exper + female)
ggPredict(model_041, se = FALSE, interactive = TRUE)

```{r sw_6_}

```

pacman::p_load(performance)

```{r sw_6_}

```

oldpar <- par( mfrow=c(2,2))
plot(modelo_publicidad3)
par(oldpar)

```{r sw_6_}

```

check_model(modelo_publicidad3)

```{r sw_6_}

```

check_autocorrelation(modelo_publicidad3)
check_collinearity(modelo_publicidad3)
check_heteroscedasticity(modelo_publicidad3)
check_normality(modelo_publicidad3)



