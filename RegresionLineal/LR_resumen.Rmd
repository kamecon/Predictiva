---
title: "Resumen laboratorios de Regresión Lineal"
author: "Kamal Romero"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  rmdformats::readthedown:
    highlight: kate
  html_document: 
    toc: true
    number_sections: true
    theme: "yeti"
  md_document:
    variant: markdown_github
  pdf_document: 
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE)
```

# Laboratorio 1

Lo primero que debemos hacer **siempre** es cargar o instalar las librerías que necesitemos para nuestro trabajo.

En este curso hemos usado la librería `pacman`, la cual no solo carga la librería sino que la instala en caso que el usuario no la tenga instalada. Esto nos ahorra no solo los pasos de instalar y cargar, sino además instala la librería.

Verificamos si esta instalado la libreria `pacman`, en caso que no lo este, se instala

```{r lib01}
if (! ('pacman' %in% installed.packages())) install.packages('pacman')
```

Una vez instalada la librería `pacman`, cargamos (o instalamos) las librerías necesarias para este laboratorio

```{r lib02}
pacman::p_load("haven", "AER")
```

## Primer ejemplo del libro de Stock y Watson

### Dos formas de cargar los datos

La primera es accediendo a la web del libro y descargar los datos. Acceder a esta [dirección](https://wps.pearsoned.com/aw_stock_ie_3/178/45691/11696965.cw/index.html) y pinchar en *"Replication Files"* en el menu de la barra izquierda, luego pinchar en *"Datasets for Replicating Empirical Results (Original Edition)"*. Descargar los datos *"California Test Score Data Used in Chapters 4-9"* en formato STATA.

Vamos a practicar con el formato de STATA por dos motivos: debemos acostumbrarnos a trabajar con distintos formatos de datos mas alla del csv y xlsx, y porque STATA (de momento) sigue siendo el software estadístico predominante en organimos multilaterales y gubernamentales economicos, asi como en gran parte de la investigacion academica en economia

Una vez descargados los datos, lo leemos con la libreria haven 

```{r sw_1_01}
caschool <- read_dta("Data/caschool.dta")
```

Vamos a replicar la regresion lineal del capitulo 4 trabajada en clase, la cual explicaba la calificación de la prueba de habilidad numerica y verbal en funcion del numero de alumnos por profesor

En R la funcion `lm()` lleva a cabo la regresion lineal, veamos la ayuda

```{r sw_1_02, eval=FALSE}
?lm
```

De momento nos concentramos en los primeros dos argumentos de la funcion, formula y data


```{r sw_1_03}
lm(data = caschool, formula = testscr ~ str)
```

Vemos que con el argumento data de la función `lm`, indicamos de que fuente de datos se deben obtener las variables, otra forma es no indicar dentro de la funcion la fuente de los datos, y usar la funcion `attach` previo a `lm`

```{r sw_1_04}
attach(caschool)
lm(testscr ~ str)
```


Asimismo, es deseable referenciar (asignar un nombre) el objeto que surge de la función `lm`

```{r sw_1_05}
estimacion_01 <- lm(testscr ~ str)
estimacion_01
```

Más adelante aprenderemos más acerca de la funcion `lm`


Para reproducir el gráfico del libro, usamos las funciones plot y abline

```{r sw_1_06}
plot(caschool$str, caschool$testscr)
abline(estimacion_01)

```

Mas parecido al gráfico del libro

```{r sw_1_07}
plot(caschool$str, caschool$testscr,
     ylim = c(600,720),   #limites del eje y
     xlim = c(10,30),     #limites del eje x
     pch = 20,            #forma del simbolo (circulo)
     cex = 1.5,           #tamaño de la forma
     ylab = "Test score", #titulo del eje y
     xlab = "Student-teacher ratio", #titulo del eje x
     axes = FALSE)        #Sin ejes
axis(1) #eje x
axis(2) #eje y
abline(estimacion_01, col = "deepskyblue", lwd =3)

```

Si leen el documento de características de los datos, podran ver que las dos variables que hemos usado no estaban en los datos originales, sino que se han construido

Esto es algo que deben hacer continuamente en cualquier trabajo empirico, por lo que deberian practicarlo

Para esto, vamos a acceder los mismos datos de otra fuente, la libreraia AER (Applied Econometrics with R), accedamos al Vignette

```{r sw_1_08}
vignette("AER")
```

Veamos las bases de datos del libro de Stock y Watson


```{r sw_1_09, echo=FALSE}
help("StockWatson2007", package = "AER")
```

Cargamos los datos 

```{r sw_1_10}
data(CASchools)
```

Construimos las variables

```{r sw_1_11}
CASchools$STR <- CASchools$students/CASchools$teachers

CASchools$Testscr <- (CASchools$math+CASchools$read)/2

lm(formula = Testscr ~ STR, data = CASchools)

```

## Regresion Lineal: Población vs muestra

El objetivo de la siguiente sección es replicar lo que ocurre si estimamos varias veces el mismo proceso, pero con distintas realizaciones del ruido. Aunque la analogía no es exacta, esto es algo similar a lo que ocurre si tomamos muestras repetidas de una población determinada, volveremos a esto en un próximo laboratorio. Ahora, lo que veremos, es que aún teniendo el modelo verdadero, el hecho de que exista un ruido en dicho modelo, hace que nuestras estimaciones sean solo una de muchas posibles

Vamos a replicar la figura 3.3 del libro An [Introduction to Statistical Learning](https://www.statlearning.com/)

Creamos la función del proceso generador de datos $Y=2+3x$

```{r sw_1_11_2}
x <- seq(-2, 2, length = 100)
y <- 2 + 3*x
```


A partir de 100 realizaciones de una normal con media cero y desviación típica (`dt`) 1, creamos el modelo $Y=f(x)+u$, donde $f(x)=2+3x$

```{r sw_1_12}
dt <- 1
y_02 <- y + rnorm(n = 100, mean = 0, sd = dt)
df <- data.frame(x,y,y_02)
```

Estimamos el modelo a partir de los datos con ruido

```{r sw_1_13}
fit1 <- lm(formula = y_02 ~ x, data = df)
fit1
```


Repetimos el procedimiento anterior con 100 realizaciones distintas del error y estimamos los parámetros

```{r sw_1_14}
y_03 <- y + rnorm(n = 100, mean = 0, sd = dt)
df$y_03 <- y_03
fit2 <- lm(formula = y_03 ~ x, data = df)
fit2
```


Lo hacemos de nuevo

```{r sw_1_15}
y_04 <- y + rnorm(n = 100, mean = 0, sd = dt)
df$y_04 <- y_04
fit3 <- lm(formula = y_04 ~ x, data = df)
fit3
```


Observamos que las estimaciones de la ordenada en el origen y la pendiente cambian con distintas realizaciones del ruido

Graficamos el proceso generador de datos "real" con las estimaciones de la regresión lineal

```{r sw_1_16}
plot(x,y_02, ylab = "Y", main = "Figura 3.3 Libro ISLR")
lines(x,y, col="red", lwd=4)
lines(x, fitted(fit1), col=colors()[10*1], lwd=2)
```


Vamos a generar 10 nuevos modelos repitiendo lo anterior

Creamos un data frame vacio de 100 filas (x's) y 10 columnas (modelos), para rellenarlo luego con las distintas versiones del modelo
```{r sw_1_17}
df2 <- data.frame(matrix(, nrow=100, ncol=10))
```

Añadimos una columna con la variable X

```{r sw_1_18}
df2$x <- x
```

Generamos los 10 distintos conjuntos de datos a partir del proceso generador de datos y el ruido

Primero la columna Y
```{r sw_1_19}
for (i in 1:10) {
  df2[,i] <- y + rnorm(100, 0, dt)
}
```


Luego estimamos los 10 modelos y lo representamos graficamente
```{r sw_1_20}
plot(x,y_02, ylab = "Y", main = "Figura 3.3 Libro ISLR")
for (i in 1:10) {
  fitt <- lm(data = df2, formula = df2[,i]~x)
  lines(x, fitted(fitt), col=colors()[12*i])
}
```


# Laboratorio 2

```{r sw_2_01}
pacman::p_load("haven", "wooldridge")
```

## Practica regresion lineal 

**Nota:**parte de esta sección se desarrolla con más detalle en el html *Regresion Lineal Simple en R*

Volvemos al ejemplo del libro de Stock y Watson trabajado en la sección anterior

```{r sw_2_02}
caschool <- read_dta("RegresionLineal/Data/caschool.dta")

estimacion_01 <- lm(formula = testscr ~str, data = caschool)

summary(estimacion_01)

```


Accedemos a los coeficientes estimado del modelo

```{r sw_2_03}
estimacion_01$coefficients
estimacion_01$coefficients[1]
estimacion_01$coefficients[2]

coef(estimacion_01)

```


**Adicional: Regresión robusta** Esto se desarrolla con más detalle en el HTML *Regresión lineal simple con errores robustos a heterocedasticidad*

```{r sw_2_04}
pacman::p_load("lmtest", "sandwich")

coeftest(estimacion_01, vcov = vcovHC(estimacion_01, type = "HC0"))

```


Accedemos a mas informacion (R cuadrado, ESR)

```{r sw_2_05}
attributes(summary(estimacion_01))

summary(estimacion_01)$r.squared
summary(estimacion_01)$sigma

```


Construimos los residuos

```{r sw_2_06}
bhat <- estimacion_01$coefficients
yhat <- estimacion_01$coefficients[1] + estimacion_01$coefficients[2]*caschool$str
residuo <- caschool$testscr - yhat

```

Lo representamos gráficamente

```{r sw_2_07}
plot(residuo, type = "l")
lines(estimacion_01$residuals, col="red")

```

Obtenemos las predicciones del modelo y los residuos, los organizamos en una tabla

```{r sw_2_08}
Str <-  caschool$str
Test_scr <-  caschool$testscr
testscr_hat <- estimacion_01$fitted.values
u_hat <- estimacion_01$residuals

```


Usamos el comando cbind() para unir las columnas (vectores) de datos que hemos creado en el paso anterior

```{r sw_2_09}
tabla01 <- cbind(Str, Test_scr, testscr_hat, u_hat)

head(tabla01)

```

A continuación, replicamos algunos de los ejemplos del libro de Wooldridge

### Ejemplo 2.3 libro Wooldridge

```{r sw_2_10}
data(ceosal1)

estimacion_wool01 <- lm(formula = salary ~roe, data = ceosal1)
estimacion_wool01

plot(ceosal1$roe, ceosal1$salary, xlab = "ROE", ylab = "Salario de los CEO's")
abline(estimacion_wool01)

```

Los siguientes dos ejemplos deben completarlos siguiendo el paso anterior

### Ejemplo 2.4 libro Wooldridge: Relacion entre salario (wage) y educacion (educ) (Tarea: inspeccionar los datos)


```{r sw_2_11}
data("wage1")

estimacion_wool02 <- lm()
estimacion_wool02

plot(, xlab = , ylab = )
abline(estimacion_wool02)

```


### Ejemplo 2.5 libro Wooldridge: Relacion entre resultado de las elecciones (voteA) y el gasto en la campaña electoral (shareA)


```{r sw_2_12}
data("vote1")

?vote1

```


## Distribucion muestral de los betas 

Al igual que en su curso pasado de estadística inferencial, en el cual se estimada un parámetro poblacional desconocido a partir de una muestra usando un estadístico, en este caso hacemos lo mismo cuando estimamos los parámetros de un modelo lineal.

Del mismo modo que podíamos construir una distribución de estadísticos muestrales, partiendo del supuesto de que tomabamos muestras repetidas de una población dada, podemos crear una distribución de los betas estimados en un modelo lineal.

A continuación vamos a tomar sub-muestras de las base de datos de caschool, y estimamos el modelo lineal $Testscr=\beta_0+\beta_1STR+u$ en cada una de ellas

Comenzamos creando un vector vacio de betas, cuya dimesión es igual al número de sub-muestras (y veces que estimamos el modelo) que tomamos

```{r sw_2_}
beta_1 <- vector(length = 1000)
```

Para tomar una muestra (en este caso de tamaño 100) de un data frame, hacemos lo siguiente

```{r sw_2_}
sample(nrow(caschool),100)
caschool[sample(nrow(caschool),100), ]

```


Rellenamos el vector con los betas estimados tomando solo una muestra de los datos originales

```{r sw_2_}
for (i in seq_along(beta_1)) {
  estima <- lm(data =  caschool[sample(nrow(caschool),100),], formula = testscr ~str)
  beta_1[i] <- estima$coefficients[2]
}

mean(beta_1)
hist(beta_1)

```


**NOTA DE R**: En lugar de la función `seq_along`se puede usar una notación más convencional en otros lenguajes como for (i in 1..1000) o for (i in 1:1000) o for i in range(1000), que significa "repite esta operación en índices que van del 1 al 1000 en saltos de 1" que es una manera sofisticada de decir "repite esta operación 1000 veces" 

**NOTA DE R**: En lugar de usar un bucle se puede usar la función `replicate`, una de las funciones de la familia `apply`, las cuales aplican una función a un vector de valores, que es lo mismo que hace un bucle cuando _aplica_ de manera repetida la misma operación a un conjunto de valores representado por un vector. En este caso estamos haciendo algo más sencillo, repetir un número de veces una instrucción encapsulada en una función, en este caso que nos extraiga de manera repetida una muestra de una población y calcule una regresion. Así que `replicate` en nuestro ejemplo repite 1000 veces la operación descrita arriba.

Este tipo de operaciones se realiza muchas veces en estadística, ya que de esa manera hacemos _simulaciones_, repetir muchas veces realizaciones de una distribución de probabilidad o un fenómeno cuyo resultado sea incierto (lanzar un dado 50000 veces)

```{r sw_2_}
betas_1_2 <- replicate(expr = lm(data =  caschool[sample(nrow(caschool),100),], formula = testscr ~str)$coefficients[2],
                          n = 1000)

mean(betas_1_2)
hist(betas_1_2)

```

# Laboratorio 3

```{r sw_3_01}
pacman::p_load("haven", "wooldridge")
```

## Regresion lineal:parametros

**Nota:**parte de esta sección se desarrolla con más detalle en el html *Regresion Lineal Simple en R*

Volvemos al ejemplo del libro de Stock y Watson trabajado en las secciones anteriores

```{r sw_3_02}
caschool <- read_dta("RegresionLineal/Data/caschool.dta")

estimacion_01 <- lm(formula = testscr ~str, data = caschool)

summary(estimacion_01)

```

Ya sabemos como acceder a los betas, el ESR y el $R^2$. Ahora vamos a acceder a los errores estándar de los parámetros, el t-valor y el p-valor

Volvemos a los atributos de la función `summary`

```{r sw3_}
attributes(summary(estimacion_01))
```

Inspeccionamos el atributo coefficients

```{r sw3_}
summary(estimacion_01)$coefficients
```


Podemos ver que es una matriz de dimension 2,4

```{r sw3_}
class(summary(estimacion_01)$coefficients)
dim(summary(estimacion_01)$coefficients)

```

Sabiendo esto, podemos extraer los elementos deseados

Error estándar de $Beta_0$

```{r sw3_}
summary(estimacion_01)$coefficients[1,2]
```

Error estándar de Beta 1

```{r sw3_}
summary(estimacion_01)$coefficients[2,2]
```

Estadístico t para contrastar que Beta 1 es 0

```{r sw3_}
summary(estimacion_01)$coefficients[2,3]
```

p-valor del contraste de Beta 1

```{r sw3_}
summary(estimacion_01)$coefficients[2,4]
```

**NOTA**: Si desea comparar el estadístico t con el valor critico, lo puede hacer con la funcion `qt()`

Al 99% de confianza (0.05 de probabilidad en cada cola en el caso del contraste bilateral), el estadistico de contraste seria


```{r sw3_}
qt(p = 0.995, df = 418)
```



Una manera de comapararlo


```{r sw3_}
t_valor <- summary(estimacion_01)$coefficients[2,3]
contraste <- qt(p = 0.995, df = 418)
ifelse(abs(t_valor)>abs(contraste),"Rechaza","Acepta")

```


Intervalos de confianza para $Beta_1$


```{r sw3_}
confint(estimacion_01)
```

Tambien se podria calcular "manualmente" lo anterior. Para eso, necesitariamos el numero de observaciones


```{r sw3_}
n <- nobs(estimacion_01)
n

```


